\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=2cm]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage[colorlinks=true]{hyperref}

\usepackage{listings}
\usepackage[font=small,labelfont=bf]{caption} % Required for specifying captions to tables and figures

\usepackage{graphicx} % Required for the inclusion of images
\graphicspath{{./images/}} % Specifies the directory where pictures are stored

\setlength{\droptitle}{-6em}

\begin{document}

\center
Aprendizagem 2024\\
Homework I -- Group 016\\
(ist1106022, ist1106720)\vskip 1cm

\large{\textbf{Part I}: Pen and paper}\normalsize

\begin{enumerate}[leftmargin=\labelsep, label=\textbf{\arabic*.)}]
    \item \begin{itemize}
              \item Data Subset for \( y_1 \geq 0.3 \) \\
                    \vspace{0.5em}
                    \begin{minipage}{0.30\textwidth}
                        \begin{itemize}
                            \item Class A $\rightarrow 3 \ (x_7, x_8, x_{11})$
                            \item Class B $\rightarrow 2 \ (x_6, x_{12})$
                            \item Class C $\rightarrow 2 \ (x_9, x_{10})$
                        \end{itemize}
                    \end{minipage}
                    \begin{minipage}{0.45\textwidth}
                        \begin{center}
                            \[\begin{array}{|c|c|c|c|c|c|}
                                    \hline
                                    D      & y_1  & y_2 & y_3 & y_4 & y_{\text{out}} \\
                                    \hline
                                    x_6    & 0.30 & 0   & 1   & 0   & B              \\
                                    x_7    & 0.76 & 0   & 1   & 1   & A              \\
                                    x_8    & 0.86 & 1   & 0   & 0   & A              \\
                                    x_9    & 0.93 & 0   & 1   & 1   & C              \\
                                    x_{10} & 0.47 & 0   & 1   & 1   & C              \\
                                    x_{11} & 0.73 & 1   & 0   & 0   & A              \\
                                    x_{12} & 0.89 & 1   & 2   & 0   & B              \\
                                    \hline
                                \end{array}\]
                        \end{center}
                    \end{minipage}

              \item Entropy Calculation for \( H(y_{\text{out}}) \)

                    \[
                        H(y_{\text{out}}) = -\frac{3}{7} \log_2 \frac{3}{7} - \frac{2}{7} \log_2 \frac{2}{7} - \frac{2}{7} \log_2 \frac{2}{7} = 1.557
                    \]

              \item Calculating \( H(y_{\text{out}}|y_2) \)

                    \[
                        H(y_{\text{out}}|y_2) = \frac{4}{7} \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{2} \log_2 \frac{1}{2}\right) + \frac{3}{7} \times \left( -\frac{2}{3} \log_2 \frac{2}{3} - \frac{1}{3} \log_2 \frac{1}{3} \right) = 1.251
                    \]

              \item Information Gain for \( y_2 \)

                    \[
                        IG(y_2) = H(y_{\text{out}}) - H(y_{\text{out}}|y_2) = 1.557 - 1.251 = 0.306
                    \]

              \item Calculating \( H(y_{\text{out}}|y_3) \)

                    \[
                        H(y_{\text{out}}|y_3) = \frac{2}{7} \times \left( -1 \log_2 1 \right) + \frac{4}{7} \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2} \right) + \frac{1}{7} \times \left( -1 \log_2 1 \right) = 0.857
                    \]

              \item Information Gain for \( y_3 \)

                    \[
                        IG(y_3) = H(y_{\text{out}}) - H(y_{\text{out}}|y_3) = 1.557 - 0.857 = 0.7
                    \]

              \item Calculating \( H(y_{\text{out}}|y_4) \)

                    \[
                        H(y_{\text{out}}|y_4) = \frac{4}{7} \times \left( -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2} \right) + \frac{3}{7} \times \left( -\frac{1}{3} \log_2 \frac{1}{3} - \frac{2}{3} \log_2 \frac{2}{3} \right) = 0.965
                    \]

              \item Information Gain for \( y_4 \)

                    \[
                        IG(y_4) = H(y_{\text{out}}) - H(y_{\text{out}}|y_4) = 1.557 - 0.965 = 0.592
                    \]

              \item Information Gain Evaluation

                    Since feature \( y_3 \) has the greatest information gain, it is selected as the splitting criterion.

                    \begin{itemize}
                        \item \( y_3 = 0 \rightarrow \text{Class A} \)
                        \item \( y_3 = 1 \rightarrow \text{Class A/B/C} \)
                        \item \( y_3 = 2 \rightarrow \text{Class B} \)
                    \end{itemize}

              \item Data Subset for \( y_1 \geq 0.3 \) and \( y_3 = 1 \) \\
                    \vspace{0.5em}
                    \begin{minipage}{0.30\textwidth}
                        \begin{itemize}
                            \item Class A $\rightarrow 1 \ (x_7)$
                            \item Class B $\rightarrow 1 \ (x_6)$
                            \item Class C $\rightarrow 2 \ (x_9, x_{10})$
                        \end{itemize}
                    \end{minipage}
                    \begin{minipage}{0.45\textwidth}
                        \[
                            \begin{array}{|c|c|c|c|c|c|}
                                \hline
                                D      & y_1  & y_2 & y_3 & y_4 & y_{\text{out}} \\
                                \hline
                                x_6    & 0.30 & 0   & 1   & 0   & B              \\
                                x_7    & 0.76 & 0   & 1   & 1   & A              \\
                                x_9    & 0.93 & 0   & 1   & 1   & C              \\
                                x_{10} & 0.47 & 0   & 1   & 1   & C              \\
                                \hline
                            \end{array}
                        \]
                    \end{minipage}

              \item Entropy Calculation for \( H(y_{\text{out}}) \)

                    \[
                        H(y_{\text{out}}) = -\frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{4} \log_2 \frac{1}{4} - \frac{1}{2} \log_2 \frac{1}{2} = 1.500
                    \]

              \item Calculating \( H(y_{\text{out}}|y_2) \)

                    \[
                        H(y_{\text{out}}|y_2) = 1 \times \left( -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{4} \log_2 \frac{1}{4} -\frac{1}{2} \log_2 \frac{1}{2}\right) = 1.500
                    \]

              \item Information Gain for \( y_2 \)

                    \[
                        IG(y_2) = H(y_{\text{out}}) - H(y_{\text{out}}|y_2) = 1.500 - 1.500 = 0
                    \]

              \item Calculating \( H(y_{\text{out}}|y_4) \)

                    \[
                        H(y_{\text{out}}|y_4) = \frac{1}{4} \times \left( - 1 \log_2 1 \right) + \frac{3}{4} \times \left( -\frac{1}{3} \log_2 \frac{1}{3} - \frac{2}{3} \log_2 \frac{2}{3} \right) = 0.689
                    \]

              \item Information Gain for \( y_4 \)

                    \[
                        IG(y_4) = H(y_{\text{out}}) - H(y_{\text{out}}|y_4) = 1.500 - 0.689 = 0.811
                    \]

              \item Information Gain Evaluation

                    Since feature \( y_4 \) has the greatest information gain, it is selected as the splitting criterion.

                    \begin{itemize}
                        \item \( y_4 = 0 \rightarrow \text{Class B} \)
                        \item \( y_3 = 1 \rightarrow \text{Class A/C} \)
                    \end{itemize}

              \item Decision Tree \\
                    Since it is not possible to create any more subsets with a minimum of 4 observations, we can build the tree, taking into account that any ties are resolved by the majority class. \\
                    \begin{center}
                        \includegraphics[width=0.8\textwidth]{decision_tree_pen_and_paper.jpg}
                        \captionof{figure}{Decision tree drawn for the pen and paper exercise 1}
                    \end{center}
          \end{itemize}
\end{enumerate}

\large{\textbf{Part II}: Programming}\normalsize

\begin{enumerate}[leftmargin=\labelsep, label=\textbf{\arabic*.)}]
    \item Applying \texttt{f\_classif} from the \texttt{sklearn} library upon the dataset (after splitting into feature data matrix and target vector) allows understanding of the discrimantive power of each feature: \vskip 0.25cm
          \begin{lstlisting}[basicstyle=\ttfamily\centering]
'Pregnancies': 39.67
'Glucose': 213.16
'BloodPressure': 3.26
'SkinThickness': 4.3
'Insulin': 13.28
'BMI': 71.77
'DiabetesPedigreeFunction': 23.87
'Age': 46.14
        \end{lstlisting}
          The scores indicate that \texttt{Glucose} is the \textbf{most discriminative} feature, whereas \texttt{BloodPressure} is the \textbf{least discriminative}.
          The following plot shows the class-condition probability density functions for these two features:\vskip 0.25cm
          \begin{center}
              \includegraphics{discrimantive_features_dense_probability.png}
          \end{center}
    \item To measure accuracy levels originated from using a decision tree with the minimums sample split values (2, 5, 10, 20, 30, 50, 100) when branching, a stratified 80-20 training-testing split was performed. \\ \vspace{0.25em} Additionally, since \texttt{sklearn} performs non-deterministic thresholding of numeric variables in decision trees, the results were made by averaging over 10 runs per parametrization (leading to the error margins in the graph). \\ \vspace{0.25em} The following graph shows the different averaged accuracy levels for the decision tree classifiers generated for each minimum split: \\
          \begin{center}
              \includegraphics{split_training_results.png}
          \end{center}
    \item The smaller decision tree minimum splits have the highest accuracy levels on the training set, while having the smaller values on the testing set, which indicates overfitting.
          The following minimum splits lead to increansigly better accuracy values on the testing set, although continuing to decrease the accuracy on the training set, which indicates the model is starting to have better generalization capabilities and less overfitting. \\\vspace{0.25em}
          Out of all the available minimum splits, the best pick would be the one leading to a smaller difference in accuracies between the two sets, which in this case would be 100. \\\vspace{0.25em}
          Ideally with higher minimum sample split values, there would be a value leading high and identical accuracy levels on both sets.
    \item \begin{enumerate}[label=\textbf{\roman*.)}]
              \item Using \textit{all} data to train a single decision tree classifier with a maximum depth of 3 leads to the following tree (\texttt{value} and \texttt{samples} expressed in proportion):\\
                    \includegraphics{decision_tree_value_counts.png}\\
                    \vspace{2em}
                    But with this second view of the same tree, we can more easily understand the probability of each class in each node:\\
                    \includegraphics{decision_tree.png} \\
                    The \textcolor{orange}{"oranger"} the node, the higher the probability of the class being 0 (eq. non-diabetic), and the \textcolor{blue}{"bluer"} the node, the higher the probability of the class being 1 (eq. diabetic). \\
              \item Overall, high \texttt{Glucose} and \texttt{BMI} are the most common indicator of diabetes. (agreeing with our analysis in \textit{exercise 1}). \\\vspace{0.25em}
                    Following is a thorough analysis of the probability in each conditional association (extracted from the graph):
                    \vspace{0.5em}
                    \begin{itemize}
                        \item $P( \text{\texttt{Diabetes}}) = 0.349$
                    \end{itemize}
                    \begin{itemize}
                        \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$}}) = 0.194$:
                              \begin{itemize}
                                  \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $\leq 28.5$}}) = 0.085$:
                                        \begin{itemize}
                                            \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $\leq 28.5$ \& BMI $\leq 45.4$}}) = 0.075$
                                            \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $\leq 28.5$ \& BMI $> 45.4$}}) = 0.75$
                                        \end{itemize}
                                  \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $> 28.5$}}) = 0.332$:
                                        \begin{itemize}
                                            \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $> 28.5$ \& BMI $\leq 26.35$}}) = 0.049$
                                            \item $P( \text{\texttt{Diabetes | Glucose $\leq 127.5$ \& Age $> 28.5$ \& BMI $> 26.35$}}) = 0.399$
                                        \end{itemize}
                              \end{itemize}
                    \end{itemize}
                    \begin{itemize}
                        \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$}}) = 0.615$:
                              \begin{itemize}
                                  \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $\leq 29.95$}}) = 0.316$:
                                        \begin{itemize}
                                            \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $\leq 29.95$ \& Glucose $\leq 145.5$}}) = 0.146$
                                            \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $\leq 29.95$ \& Glucose $> 145.5$}}) = 0.514$
                                        \end{itemize}
                                  \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $> 29.95$}}) = 0.725$:
                                        \begin{itemize}
                                            \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $> 29.95$ \& Glucose $\leq 157.5$}}) = 0.609$
                                            \item $P( \text{\texttt{Diabetes | Glucose $> 127.5$ \& BMI $> 29.95$ \& Glucose $> 157.5$}}) = 0.87$
                                        \end{itemize}
                              \end{itemize}
                    \end{itemize}
                    \vspace{0.5em}
                    From the analysis it is possible to easily understand the likelihood of a new patient, given his retrieved medical data, to have diabetes or not.
          \end{enumerate}
\end{enumerate}
\end{document}
